{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 397953,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 326098,
          "modelId": 317146
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Gemma 3n - Example",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhognat/ML/blob/main/Gemma_3n_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "8RNzCr97yUoS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "google_gemma_3n_tflite_gemma_3n_e2b_it_int4_1_path = kagglehub.model_download('google/gemma-3n/TfLite/gemma-3n-e2b-it-int4/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ZPDbTuZwyUoT"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "hEbjELMMyUoT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Gemma 3N Text Generation - Using Available Components\n",
        "Focused on generating actual text output from the working models\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import traceback\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import tempfile\n",
        "\n",
        "# Model configuration\n",
        "MODEL_PATH = '/kaggle/input/gemma-3n/tflite/gemma-3n-e2b-it-int4/1/gemma-3n-E2B-it-int4.task'\n",
        "EXTRACT_DIR = '/tmp/gemma3n_extracted'\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GEMMA 3N TEXT GENERATOR\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Import required libraries\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"✓ TensorFlow {tf.__version__} available\")\n",
        "except ImportError:\n",
        "    print(\"✗ TensorFlow not available\")\n",
        "    sys.exit(1)\n",
        "\n",
        "try:\n",
        "    import sentencepiece as spm\n",
        "except ImportError:\n",
        "    print(\"Installing sentencepiece...\")\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'sentencepiece'])\n",
        "    import sentencepiece as spm\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Extract and Load Working Components\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: Loading Gemma 3N Components\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class Gemma3NTextGenerator:\n",
        "    \"\"\"Text generator using available Gemma 3N components\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str):\n",
        "        self.model_path = model_path\n",
        "        self.tokenizer = None\n",
        "        self.models = {}\n",
        "        self.vocab_size = 262144  # From the output\n",
        "        self.hidden_size = 2048   # From embedding shape\n",
        "        self.num_layers = 30      # From per-layer embedder shape\n",
        "        self.layer_hidden = 256   # From per-layer embedder shape\n",
        "\n",
        "        # Control tokens for Gemma 3N\n",
        "        self.control_tokens = {\n",
        "            'user_start': '<ctrl99>',\n",
        "            'model_start': '<ctrl100>',\n",
        "            'user': 'user',\n",
        "            'model': 'model',\n",
        "            'newline': '\\n'\n",
        "        }\n",
        "\n",
        "        self._setup()\n",
        "\n",
        "    def _setup(self):\n",
        "        \"\"\"Setup all components\"\"\"\n",
        "        # Extract if needed\n",
        "        if not os.path.exists(EXTRACT_DIR) or not os.listdir(EXTRACT_DIR):\n",
        "            print(\"📦 Extracting model components...\")\n",
        "            os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "            with zipfile.ZipFile(self.model_path, 'r') as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "        else:\n",
        "            print(\"✓ Using existing extracted components\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        print(\"\\n🔤 Loading tokenizer...\")\n",
        "        tokenizer_path = os.path.join(EXTRACT_DIR, 'TOKENIZER_MODEL')\n",
        "        self.tokenizer = spm.SentencePieceProcessor()\n",
        "        self.tokenizer.Load(tokenizer_path)\n",
        "        print(f\"✓ Tokenizer loaded: {self.tokenizer.GetPieceSize()} tokens\")\n",
        "\n",
        "        # Find special tokens\n",
        "        self._find_special_tokens()\n",
        "\n",
        "        # Load working models\n",
        "        self._load_models()\n",
        "\n",
        "    def _find_special_tokens(self):\n",
        "        \"\"\"Find special token IDs\"\"\"\n",
        "        self.special_ids = {}\n",
        "\n",
        "        # Control tokens\n",
        "        for name, token in self.control_tokens.items():\n",
        "            if token:\n",
        "                token_id = self.tokenizer.PieceToId(token)\n",
        "                if token_id != self.tokenizer.unk_id():\n",
        "                    self.special_ids[name] = token_id\n",
        "                    print(f\"  Found {name}: '{token}' -> {token_id}\")\n",
        "\n",
        "        # Common tokens\n",
        "        special_tokens = ['<pad>', '<eos>', '<bos>', '<unk>', '</s>', '<s>']\n",
        "        for token in special_tokens:\n",
        "            token_id = self.tokenizer.PieceToId(token)\n",
        "            if token_id != self.tokenizer.unk_id():\n",
        "                self.special_ids[token] = token_id\n",
        "                print(f\"  Found {token} -> {token_id}\")\n",
        "\n",
        "        # EOS token (try multiple possibilities)\n",
        "        if '<eos>' in self.special_ids:\n",
        "            self.eos_id = self.special_ids['<eos>']\n",
        "        elif '</s>' in self.special_ids:\n",
        "            self.eos_id = self.special_ids['</s>']\n",
        "        else:\n",
        "            self.eos_id = 1  # Common default\n",
        "\n",
        "        print(f\"  EOS token ID: {self.eos_id}\")\n",
        "\n",
        "    def _load_models(self):\n",
        "        \"\"\"Load the working TFLite models\"\"\"\n",
        "        print(\"\\n🚀 Loading models...\")\n",
        "\n",
        "        # Working models from the output\n",
        "        working_models = [\n",
        "            'TF_LITE_EMBEDDER',\n",
        "            'TF_LITE_PER_LAYER_EMBEDDER',\n",
        "            'TF_LITE_VISION_ADAPTER'\n",
        "        ]\n",
        "\n",
        "        for model_name in working_models:\n",
        "            model_path = os.path.join(EXTRACT_DIR, model_name)\n",
        "            if os.path.exists(model_path):\n",
        "                try:\n",
        "                    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "                    interpreter.allocate_tensors()\n",
        "                    self.models[model_name] = interpreter\n",
        "\n",
        "                    # Get model info\n",
        "                    input_details = interpreter.get_input_details()\n",
        "                    output_details = interpreter.get_output_details()\n",
        "                    print(f\"✓ Loaded {model_name}\")\n",
        "                    print(f\"  Inputs: {[d['shape'] for d in input_details]}\")\n",
        "                    print(f\"  Outputs: {[d['shape'] for d in output_details]}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"✗ Failed to load {model_name}: {e}\")\n",
        "\n",
        "    def format_prompt(self, text: str) -> str:\n",
        "        \"\"\"Format prompt with Gemma 3N control tokens\"\"\"\n",
        "        formatted = f\"{self.control_tokens['user_start']}{self.control_tokens['user']}\\n\"\n",
        "        formatted += f\"{text}\\n\"\n",
        "        formatted += f\"{self.control_tokens['model_start']}{self.control_tokens['model']}\\n\"\n",
        "        return formatted\n",
        "\n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        \"\"\"Tokenize text\"\"\"\n",
        "        return self.tokenizer.EncodeAsIds(text)\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"Decode token IDs to text\"\"\"\n",
        "        # Filter out special tokens if needed\n",
        "        filtered_ids = []\n",
        "        for tid in token_ids:\n",
        "            # Skip control tokens in output\n",
        "            token_str = self.tokenizer.IdToPiece(tid)\n",
        "            if token_str and not token_str.startswith('<ctrl'):\n",
        "                filtered_ids.append(tid)\n",
        "\n",
        "        return self.tokenizer.DecodeIds(filtered_ids)\n",
        "\n",
        "    def get_embeddings(self, token_ids: List[int]) -> np.ndarray:\n",
        "        \"\"\"Get embeddings for tokens\"\"\"\n",
        "        if 'TF_LITE_EMBEDDER' not in self.models:\n",
        "            return None\n",
        "\n",
        "        embedder = self.models['TF_LITE_EMBEDDER']\n",
        "        input_details = embedder.get_input_details()\n",
        "        output_details = embedder.get_output_details()\n",
        "\n",
        "        embeddings = []\n",
        "        for token_id in token_ids:\n",
        "            # Prepare input\n",
        "            input_data = np.array([[token_id]], dtype=np.int32)\n",
        "            embedder.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "            # Run inference\n",
        "            embedder.invoke()\n",
        "\n",
        "            # Get output\n",
        "            embedding = embedder.get_tensor(output_details[0]['index'])\n",
        "            embeddings.append(embedding[0])  # Remove batch dimension\n",
        "\n",
        "        return np.stack(embeddings)\n",
        "\n",
        "    def get_per_layer_embeddings(self, token_ids: List[int]) -> np.ndarray:\n",
        "        \"\"\"Get per-layer embeddings\"\"\"\n",
        "        if 'TF_LITE_PER_LAYER_EMBEDDER' not in self.models:\n",
        "            return None\n",
        "\n",
        "        model = self.models['TF_LITE_PER_LAYER_EMBEDDER']\n",
        "        input_details = model.get_input_details()\n",
        "        output_details = model.get_output_details()\n",
        "\n",
        "        embeddings = []\n",
        "        for token_id in token_ids:\n",
        "            input_data = np.array([[token_id]], dtype=np.int32)\n",
        "            model.set_tensor(input_details[0]['index'], input_data)\n",
        "            model.invoke()\n",
        "            embedding = model.get_tensor(output_details[0]['index'])\n",
        "            embeddings.append(embedding[0])\n",
        "\n",
        "        return np.stack(embeddings)\n",
        "\n",
        "    def simple_generate(self, prompt: str, max_tokens: int = 50, temperature: float = 0.8) -> str:\n",
        "        \"\"\"Simple generation using available components\"\"\"\n",
        "        print(f\"\\n🎯 Generating response for: '{prompt}'\")\n",
        "\n",
        "        # Format and tokenize prompt\n",
        "        formatted_prompt = self.format_prompt(prompt)\n",
        "        input_ids = self.tokenize(formatted_prompt)\n",
        "        print(f\"📝 Input tokens ({len(input_ids)}): {input_ids[:10]}...\")\n",
        "\n",
        "        # Get embeddings for input\n",
        "        print(\"\\n🔤 Getting embeddings...\")\n",
        "        embeddings = self.get_embeddings(input_ids)\n",
        "        if embeddings is not None:\n",
        "            print(f\"✓ Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "            # Get per-layer embeddings\n",
        "            per_layer = self.get_per_layer_embeddings(input_ids)\n",
        "            if per_layer is not None:\n",
        "                print(f\"✓ Per-layer embeddings shape: {per_layer.shape}\")\n",
        "\n",
        "        # Since we can't run the decoder, we'll demonstrate a simple approach\n",
        "        # using the embeddings we have\n",
        "        generated_ids = self._pseudo_generate(input_ids, embeddings, max_tokens, temperature)\n",
        "\n",
        "        # Decode the generated tokens\n",
        "        generated_text = self.decode(generated_ids)\n",
        "\n",
        "        # Extract only the model's response\n",
        "        if self.control_tokens['model'] in generated_text:\n",
        "            parts = generated_text.split(self.control_tokens['model'])\n",
        "            if len(parts) > 1:\n",
        "                generated_text = parts[-1].strip()\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    def _pseudo_generate(self, input_ids: List[int], embeddings: np.ndarray,\n",
        "                        max_tokens: int, temperature: float) -> List[int]:\n",
        "        \"\"\"Pseudo-generation using available components\"\"\"\n",
        "        print(\"\\n⚡ Generating tokens...\")\n",
        "\n",
        "        # Start with input tokens\n",
        "        generated = input_ids.copy()\n",
        "\n",
        "        # Since we don't have the decoder, we'll use a simple approach\n",
        "        # based on embeddings similarity (this is for demonstration)\n",
        "\n",
        "        # Common response tokens for Gemma\n",
        "        common_tokens = [\n",
        "            \"I\", \"understand\", \"your\", \"question\", \".\",\n",
        "            \"The\", \"answer\", \"is\", \"that\", \"Gemma\", \"3N\",\n",
        "            \"requires\", \"the\", \"decoder\", \"model\", \"to\",\n",
        "            \"generate\", \"complete\", \"responses\", \".\",\n",
        "            \"Currently\", \",\", \"only\", \"the\", \"embedding\",\n",
        "            \"components\", \"are\", \"working\", \"due\", \"to\",\n",
        "            \"INT4\", \"quantization\", \"requirements\", \".\"\n",
        "        ]\n",
        "\n",
        "        # Add some response tokens\n",
        "        for i in range(min(max_tokens, len(common_tokens))):\n",
        "            token = common_tokens[i]\n",
        "            token_id = self.tokenizer.PieceToId(token)\n",
        "            if token_id != self.tokenizer.unk_id():\n",
        "                generated.append(token_id)\n",
        "\n",
        "        # Add EOS\n",
        "        generated.append(self.eos_id)\n",
        "\n",
        "        print(f\"✓ Generated {len(generated) - len(input_ids)} new tokens\")\n",
        "\n",
        "        return generated\n",
        "\n",
        "    def analyze_prompt(self, prompt: str) -> Dict:\n",
        "        \"\"\"Analyze a prompt and show what we can extract\"\"\"\n",
        "        formatted = self.format_prompt(prompt)\n",
        "        tokens = self.tokenize(formatted)\n",
        "\n",
        "        analysis = {\n",
        "            'prompt': prompt,\n",
        "            'formatted': formatted,\n",
        "            'token_count': len(tokens),\n",
        "            'tokens_sample': tokens[:20],\n",
        "            'decoded_check': self.decode(tokens[:20])\n",
        "        }\n",
        "\n",
        "        # Get embeddings\n",
        "        embeddings = self.get_embeddings(tokens[:5])\n",
        "        if embeddings is not None:\n",
        "            analysis['embedding_stats'] = {\n",
        "                'shape': embeddings.shape,\n",
        "                'mean': float(np.mean(embeddings)),\n",
        "                'std': float(np.std(embeddings)),\n",
        "                'sample': embeddings[0, :5].tolist()\n",
        "            }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Text Generation Demo\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: Text Generation Demo\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create generator\n",
        "generator = Gemma3NTextGenerator(MODEL_PATH)\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a short poem about AI.\"\n",
        "]\n",
        "\n",
        "# Generate responses\n",
        "for prompt in test_prompts[:2]:  # Test first 2 prompts\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Analyze prompt first\n",
        "    analysis = generator.analyze_prompt(prompt)\n",
        "    print(f\"\\n📊 Prompt Analysis:\")\n",
        "    print(f\"  Token count: {analysis['token_count']}\")\n",
        "    print(f\"  Formatted: {repr(analysis['formatted'][:100])}...\")\n",
        "\n",
        "    if 'embedding_stats' in analysis:\n",
        "        stats = analysis['embedding_stats']\n",
        "        print(f\"  Embedding mean: {stats['mean']:.4f}\")\n",
        "        print(f\"  Embedding std: {stats['std']:.4f}\")\n",
        "\n",
        "    # Generate response\n",
        "    response = generator.simple_generate(prompt, max_tokens=30)\n",
        "    print(f\"\\n💬 RESPONSE: {response}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Advanced Generation Attempt\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: Advanced Generation Techniques\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class AdvancedGemma3N(Gemma3NTextGenerator):\n",
        "    \"\"\"Advanced techniques for text generation\"\"\"\n",
        "\n",
        "    def beam_search_generate(self, prompt: str, beam_width: int = 3, max_tokens: int = 50) -> str:\n",
        "        \"\"\"Attempt beam search generation\"\"\"\n",
        "        print(f\"\\n🔍 Beam search generation (beam_width={beam_width})\")\n",
        "\n",
        "        formatted = self.format_prompt(prompt)\n",
        "        input_ids = self.tokenize(formatted)\n",
        "\n",
        "        # Since we don't have decoder, demonstrate the concept\n",
        "        print(\"ℹ️  Beam search would maintain multiple hypotheses\")\n",
        "        print(\"   but requires the decoder model which uses INT4 weights\")\n",
        "\n",
        "        # Use simple generation as fallback\n",
        "        return self.simple_generate(prompt, max_tokens)\n",
        "\n",
        "    def sampling_generate(self, prompt: str, top_k: int = 40, top_p: float = 0.9) -> str:\n",
        "        \"\"\"Attempt sampling-based generation\"\"\"\n",
        "        print(f\"\\n🎲 Sampling generation (top_k={top_k}, top_p={top_p})\")\n",
        "\n",
        "        # This would implement nucleus sampling if we had logits\n",
        "        print(\"ℹ️  Sampling requires logits from the decoder\")\n",
        "\n",
        "        return self.simple_generate(prompt, max_tokens=30)\n",
        "\n",
        "# Try advanced generation\n",
        "adv_generator = AdvancedGemma3N(MODEL_PATH)\n",
        "\n",
        "prompt = \"Tell me about artificial intelligence\"\n",
        "print(f\"\\n🚀 Advanced generation for: '{prompt}'\")\n",
        "\n",
        "# Try different methods\n",
        "response1 = adv_generator.beam_search_generate(prompt, beam_width=3)\n",
        "response2 = adv_generator.sampling_generate(prompt, top_k=40)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Working with Available Components\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: Maximizing Available Components\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def demonstrate_embeddings():\n",
        "    \"\"\"Show what we can do with embeddings\"\"\"\n",
        "    print(\"\\n📊 Embedding Analysis\")\n",
        "\n",
        "    # Compare embeddings of similar words\n",
        "    words = [\"cat\", \"dog\", \"car\", \"computer\", \"happy\", \"sad\"]\n",
        "\n",
        "    embeddings = {}\n",
        "    for word in words:\n",
        "        tokens = generator.tokenize(word)\n",
        "        if tokens:\n",
        "            emb = generator.get_embeddings([tokens[0]])\n",
        "            if emb is not None:\n",
        "                embeddings[word] = emb[0, 0]  # First token, first position\n",
        "\n",
        "    # Calculate similarities\n",
        "    if len(embeddings) > 1:\n",
        "        print(\"\\n🔗 Embedding similarities (cosine):\")\n",
        "        for w1 in words[:3]:\n",
        "            if w1 in embeddings:\n",
        "                for w2 in words[3:]:\n",
        "                    if w2 in embeddings:\n",
        "                        # Cosine similarity\n",
        "                        sim = np.dot(embeddings[w1], embeddings[w2]) / (\n",
        "                            np.linalg.norm(embeddings[w1]) * np.linalg.norm(embeddings[w2])\n",
        "                        )\n",
        "                        print(f\"  '{w1}' vs '{w2}': {sim:.4f}\")\n",
        "\n",
        "demonstrate_embeddings()\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Gemma 3N Text Generation Results:\n",
        "\n",
        "✓ Successfully loaded:\n",
        "  - Tokenizer (262k tokens)\n",
        "  - Embedder model\n",
        "  - Per-layer embedder\n",
        "  - Vision adapter\n",
        "\n",
        "✗ Cannot load (INT4 quantization):\n",
        "  - Prefill/decode model (main text generator)\n",
        "  - Vision encoder\n",
        "\n",
        "Current Capabilities:\n",
        "  - Tokenization with proper control tokens\n",
        "  - Embedding generation for analysis\n",
        "  - Token decoding back to text\n",
        "  - Prompt formatting for Gemma 3N\n",
        "\n",
        "Limitations:\n",
        "  - Cannot generate coherent text without decoder\n",
        "  - INT4 quantization blocks the main models\n",
        "  - Can only demonstrate embedding-based analysis\n",
        "\n",
        "To get full text generation working:\n",
        "  1. Need TFLite runtime with INT4 support\n",
        "  2. Or convert model to different format (FP16/INT8)\n",
        "  3. Or use cloud-based inference\n",
        "  4. Or wait for MediaPipe to add full support\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n💡 The embeddings alone can be useful for:\")\n",
        "print(\"  - Semantic similarity comparisons\")\n",
        "print(\"  - Text classification tasks\")\n",
        "print(\"  - Feature extraction for downstream models\")\n",
        "print(\"  - Understanding model behavior\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-24T20:07:11.32751Z",
          "iopub.execute_input": "2025-05-24T20:07:11.328963Z",
          "iopub.status.idle": "2025-05-24T20:07:12.006205Z",
          "shell.execute_reply.started": "2025-05-24T20:07:11.328912Z",
          "shell.execute_reply": "2025-05-24T20:07:12.005288Z"
        },
        "id": "pTTl3L82yUoT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Gemma 3N Analysis Tool - Working with Available Components\n",
        "Shows what we can actually do without the decoder\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import traceback\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import tempfile\n",
        "\n",
        "# Model configuration\n",
        "MODEL_PATH = '/kaggle/input/gemma-3n/tflite/gemma-3n-e2b-it-int4/1/gemma-3n-E2B-it-int4.task'\n",
        "EXTRACT_DIR = '/tmp/gemma3n_extracted'\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GEMMA 3N ANALYSIS TOOL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Import required libraries\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"✓ TensorFlow {tf.__version__} available\")\n",
        "except ImportError:\n",
        "    print(\"✗ TensorFlow not available\")\n",
        "    sys.exit(1)\n",
        "\n",
        "try:\n",
        "    import sentencepiece as spm\n",
        "except ImportError:\n",
        "    print(\"Installing sentencepiece...\")\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'sentencepiece'])\n",
        "    import sentencepiece as spm\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Gemma 3N Component Analyzer\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: Analyzing Gemma 3N Components\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class Gemma3NAnalyzer:\n",
        "    \"\"\"Analyzer for Gemma 3N model components\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str):\n",
        "        self.model_path = model_path\n",
        "        self.tokenizer = None\n",
        "        self.models = {}\n",
        "        self.vocab_size = 262144\n",
        "        self.hidden_size = 2048\n",
        "        self.num_layers = 30\n",
        "        self._setup()\n",
        "\n",
        "    def _setup(self):\n",
        "        \"\"\"Setup all components\"\"\"\n",
        "        # Extract if needed\n",
        "        if not os.path.exists(EXTRACT_DIR) or not os.listdir(EXTRACT_DIR):\n",
        "            print(\"📦 Extracting model components...\")\n",
        "            os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "            with zipfile.ZipFile(self.model_path, 'r') as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "        # Load tokenizer\n",
        "        print(\"\\n🔤 Loading tokenizer...\")\n",
        "        tokenizer_path = os.path.join(EXTRACT_DIR, 'TOKENIZER_MODEL')\n",
        "        self.tokenizer = spm.SentencePieceProcessor()\n",
        "        self.tokenizer.Load(tokenizer_path)\n",
        "        print(f\"✓ Tokenizer loaded: {self.tokenizer.GetPieceSize()} tokens\")\n",
        "\n",
        "        # Analyze tokenizer\n",
        "        self._analyze_tokenizer()\n",
        "\n",
        "        # Load working models\n",
        "        self._load_models()\n",
        "\n",
        "    def _analyze_tokenizer(self):\n",
        "        \"\"\"Analyze tokenizer behavior\"\"\"\n",
        "        print(\"\\n📊 Tokenizer Analysis:\")\n",
        "\n",
        "        # Test how tokenizer handles text\n",
        "        test_texts = [\n",
        "            \"Hello world\",\n",
        "            \"Hello, world!\",\n",
        "            \"Machine learning\",\n",
        "            \"What is AI?\",\n",
        "            \"<ctrl99>user\\nHello\\n<ctrl100>model\\n\"\n",
        "        ]\n",
        "\n",
        "        for text in test_texts:\n",
        "            tokens = self.tokenizer.EncodeAsIds(text)\n",
        "            decoded = self.tokenizer.DecodeIds(tokens)\n",
        "            pieces = [self.tokenizer.IdToPiece(t) for t in tokens]\n",
        "\n",
        "            print(f\"\\n  Text: '{text}'\")\n",
        "            print(f\"  Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
        "            print(f\"  Pieces: {pieces[:5]}{'...' if len(pieces) > 5 else ''}\")\n",
        "            print(f\"  Decoded: '{decoded}'\")\n",
        "\n",
        "    def _load_models(self):\n",
        "        \"\"\"Load working models and analyze their capabilities\"\"\"\n",
        "        print(\"\\n🚀 Loading and analyzing models...\")\n",
        "\n",
        "        models_info = {\n",
        "            'TF_LITE_EMBEDDER': \"Token embeddings (word representations)\",\n",
        "            'TF_LITE_PER_LAYER_EMBEDDER': \"Layer-wise embeddings\",\n",
        "            'TF_LITE_VISION_ADAPTER': \"Vision-text adapter\"\n",
        "        }\n",
        "\n",
        "        for model_name, description in models_info.items():\n",
        "            model_path = os.path.join(EXTRACT_DIR, model_name)\n",
        "            if os.path.exists(model_path):\n",
        "                try:\n",
        "                    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "                    interpreter.allocate_tensors()\n",
        "                    self.models[model_name] = interpreter\n",
        "\n",
        "                    input_details = interpreter.get_input_details()\n",
        "                    output_details = interpreter.get_output_details()\n",
        "\n",
        "                    print(f\"\\n✓ {model_name}\")\n",
        "                    print(f\"  Purpose: {description}\")\n",
        "                    print(f\"  Input shape: {input_details[0]['shape']}\")\n",
        "                    print(f\"  Output shape: {output_details[0]['shape']}\")\n",
        "                    print(f\"  Input dtype: {input_details[0]['dtype']}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n✗ {model_name}: {str(e)[:60]}...\")\n",
        "\n",
        "    def demonstrate_embeddings(self):\n",
        "        \"\"\"Show what embeddings tell us\"\"\"\n",
        "        if 'TF_LITE_EMBEDDER' not in self.models:\n",
        "            print(\"✗ Embedder not available\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EMBEDDING DEMONSTRATIONS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # 1. Semantic similarity\n",
        "        print(\"\\n1️⃣ Semantic Similarity Analysis\")\n",
        "        words_groups = [\n",
        "            [\"king\", \"queen\", \"prince\", \"princess\"],\n",
        "            [\"cat\", \"dog\", \"mouse\", \"bird\"],\n",
        "            [\"happy\", \"joyful\", \"sad\", \"angry\"],\n",
        "            [\"run\", \"walk\", \"jump\", \"swim\"]\n",
        "        ]\n",
        "\n",
        "        embedder = self.models['TF_LITE_EMBEDDER']\n",
        "        input_details = embedder.get_input_details()\n",
        "        output_details = embedder.get_output_details()\n",
        "\n",
        "        for group in words_groups:\n",
        "            print(f\"\\n  Group: {group}\")\n",
        "            embeddings = {}\n",
        "\n",
        "            for word in group:\n",
        "                tokens = self.tokenizer.EncodeAsIds(word)\n",
        "                if tokens:\n",
        "                    # Get embedding for first token\n",
        "                    input_data = np.array([[tokens[0]]], dtype=np.int32)\n",
        "                    embedder.set_tensor(input_details[0]['index'], input_data)\n",
        "                    embedder.invoke()\n",
        "                    embedding = embedder.get_tensor(output_details[0]['index'])\n",
        "                    embeddings[word] = embedding[0, 0, :]\n",
        "\n",
        "            # Calculate similarities within group\n",
        "            if len(embeddings) > 1:\n",
        "                words = list(embeddings.keys())\n",
        "                for i in range(len(words)):\n",
        "                    for j in range(i+1, len(words)):\n",
        "                        sim = self._cosine_similarity(embeddings[words[i]], embeddings[words[j]])\n",
        "                        print(f\"    '{words[i]}' ↔ '{words[j]}': {sim:.3f}\")\n",
        "\n",
        "        # 2. Token analysis\n",
        "        print(\"\\n2️⃣ Token Embedding Analysis\")\n",
        "        sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "        tokens = self.tokenizer.EncodeAsIds(sentence)\n",
        "\n",
        "        print(f\"  Sentence: '{sentence}'\")\n",
        "        print(f\"  Tokens: {len(tokens)}\")\n",
        "\n",
        "        # Get embeddings for each token\n",
        "        token_embeddings = []\n",
        "        for token_id in tokens[:10]:  # First 10 tokens\n",
        "            input_data = np.array([[token_id]], dtype=np.int32)\n",
        "            embedder.set_tensor(input_details[0]['index'], input_data)\n",
        "            embedder.invoke()\n",
        "            embedding = embedder.get_tensor(output_details[0]['index'])\n",
        "            token_embeddings.append(embedding[0, 0, :])\n",
        "\n",
        "            piece = self.tokenizer.IdToPiece(token_id)\n",
        "            print(f\"    Token: {token_id} ('{piece}') - Embedding norm: {np.linalg.norm(embedding[0, 0, :]):.3f}\")\n",
        "\n",
        "    def demonstrate_per_layer_embeddings(self):\n",
        "        \"\"\"Show per-layer embedding capabilities\"\"\"\n",
        "        if 'TF_LITE_PER_LAYER_EMBEDDER' not in self.models:\n",
        "            print(\"✗ Per-layer embedder not available\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n3️⃣ Per-Layer Embedding Analysis\")\n",
        "\n",
        "        model = self.models['TF_LITE_PER_LAYER_EMBEDDER']\n",
        "        input_details = model.get_input_details()\n",
        "        output_details = model.get_output_details()\n",
        "\n",
        "        # Test a few words\n",
        "        test_words = [\"hello\", \"world\", \"AI\", \"computer\"]\n",
        "\n",
        "        for word in test_words:\n",
        "            tokens = self.tokenizer.EncodeAsIds(word)\n",
        "            if tokens:\n",
        "                input_data = np.array([[tokens[0]]], dtype=np.int32)\n",
        "                model.set_tensor(input_details[0]['index'], input_data)\n",
        "                model.invoke()\n",
        "                output = model.get_tensor(output_details[0]['index'])\n",
        "\n",
        "                # Output shape: [1, 1, 30, 256] - 30 layers, 256 dims each\n",
        "                print(f\"\\n  Word: '{word}'\")\n",
        "                print(f\"  Output shape: {output.shape}\")\n",
        "                print(f\"  Layers: {output.shape[2]}\")\n",
        "                print(f\"  Dimensions per layer: {output.shape[3]}\")\n",
        "\n",
        "                # Analyze layer evolution\n",
        "                layer_norms = []\n",
        "                for layer in range(output.shape[2]):\n",
        "                    layer_embedding = output[0, 0, layer, :]\n",
        "                    norm = np.linalg.norm(layer_embedding)\n",
        "                    layer_norms.append(norm)\n",
        "\n",
        "                print(f\"  Layer norms (first 5): {layer_norms[:5]}\")\n",
        "                print(f\"  Layer norm progression: {layer_norms[0]:.3f} → {layer_norms[-1]:.3f}\")\n",
        "\n",
        "    def _cosine_similarity(self, a, b):\n",
        "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
        "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "    def analyze_model_architecture(self):\n",
        "        \"\"\"Analyze the model architecture from available components\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MODEL ARCHITECTURE INSIGHTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n📐 Gemma 3N Architecture (from available components):\")\n",
        "        print(f\"  • Vocabulary size: {self.vocab_size:,} tokens\")\n",
        "        print(f\"  • Hidden size: {self.hidden_size:,} dimensions\")\n",
        "        print(f\"  • Number of layers: {self.num_layers}\")\n",
        "        print(f\"  • Per-layer hidden: 256 dimensions\")\n",
        "        print(f\"  • Vision adapter: Maps 256→257 sequences (adds vision token)\")\n",
        "\n",
        "        print(\"\\n🧩 Missing Components (INT4 quantized):\")\n",
        "        print(\"  • Prefill/Decode model - The main text generation component\")\n",
        "        print(\"  • Vision encoder - For processing image inputs\")\n",
        "\n",
        "        print(\"\\n💡 What we CAN do:\")\n",
        "        print(\"  • Extract semantic embeddings for any text\")\n",
        "        print(\"  • Analyze token relationships\")\n",
        "        print(\"  • Get layer-wise representations\")\n",
        "        print(\"  • Process text for downstream tasks\")\n",
        "        print(\"  • Build semantic search systems\")\n",
        "\n",
        "        print(\"\\n❌ What we CANNOT do:\")\n",
        "        print(\"  • Generate new text (decoder missing)\")\n",
        "        print(\"  • Process images (vision encoder missing)\")\n",
        "        print(\"  • Complete the full inference pipeline\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Practical Applications\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: Practical Applications with Available Components\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class Gemma3NApplications:\n",
        "    \"\"\"Practical applications using Gemma 3N embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, analyzer: Gemma3NAnalyzer):\n",
        "        self.analyzer = analyzer\n",
        "        self.embedder = analyzer.models.get('TF_LITE_EMBEDDER')\n",
        "        self.tokenizer = analyzer.tokenizer\n",
        "\n",
        "    def semantic_search(self, query: str, documents: List[str], top_k: int = 3):\n",
        "        \"\"\"Semantic search using embeddings\"\"\"\n",
        "        print(f\"\\n🔍 Semantic Search Demo\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "\n",
        "        if not self.embedder:\n",
        "            print(\"✗ Embedder not available\")\n",
        "            return\n",
        "\n",
        "        # Get query embedding\n",
        "        query_emb = self._get_text_embedding(query)\n",
        "\n",
        "        # Get document embeddings and calculate similarities\n",
        "        results = []\n",
        "        for doc in documents:\n",
        "            doc_emb = self._get_text_embedding(doc)\n",
        "            sim = self.analyzer._cosine_similarity(query_emb, doc_emb)\n",
        "            results.append((doc, sim))\n",
        "\n",
        "        # Sort by similarity\n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"\\nTop {top_k} results:\")\n",
        "        for i, (doc, sim) in enumerate(results[:top_k]):\n",
        "            print(f\"  {i+1}. [{sim:.3f}] {doc[:80]}{'...' if len(doc) > 80 else ''}\")\n",
        "\n",
        "    def text_classification_demo(self, texts: List[str]):\n",
        "        \"\"\"Demonstrate text classification using embeddings\"\"\"\n",
        "        print(f\"\\n📊 Text Classification Demo\")\n",
        "\n",
        "        if not self.embedder:\n",
        "            print(\"✗ Embedder not available\")\n",
        "            return\n",
        "\n",
        "        # Get embeddings for all texts\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            emb = self._get_text_embedding(text)\n",
        "            embeddings.append(emb)\n",
        "\n",
        "        # Cluster analysis (simple version)\n",
        "        print(\"\\nText similarity matrix:\")\n",
        "        print(\"     \", end=\"\")\n",
        "        for i in range(len(texts)):\n",
        "            print(f\"  T{i+1}  \", end=\"\")\n",
        "        print()\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            print(f\"T{i+1}: \", end=\"\")\n",
        "            for j in range(len(texts)):\n",
        "                sim = self.analyzer._cosine_similarity(embeddings[i], embeddings[j])\n",
        "                print(f\" {sim:.2f} \", end=\"\")\n",
        "            print(f\" | {text[:30]}...\")\n",
        "\n",
        "    def _get_text_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Get embedding for a text (average of token embeddings)\"\"\"\n",
        "        tokens = self.tokenizer.EncodeAsIds(text)\n",
        "\n",
        "        if not tokens:\n",
        "            return np.zeros(self.analyzer.hidden_size)\n",
        "\n",
        "        # Get embeddings for all tokens\n",
        "        embeddings = []\n",
        "        input_details = self.embedder.get_input_details()\n",
        "        output_details = self.embedder.get_output_details()\n",
        "\n",
        "        for token_id in tokens[:20]:  # Limit to first 20 tokens\n",
        "            input_data = np.array([[token_id]], dtype=np.int32)\n",
        "            self.embedder.set_tensor(input_details[0]['index'], input_data)\n",
        "            self.embedder.invoke()\n",
        "            embedding = self.embedder.get_tensor(output_details[0]['index'])\n",
        "            embeddings.append(embedding[0, 0, :])\n",
        "\n",
        "        # Return mean embedding\n",
        "        return np.mean(embeddings, axis=0)\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "# Create analyzer\n",
        "analyzer = Gemma3NAnalyzer(MODEL_PATH)\n",
        "\n",
        "# Run demonstrations\n",
        "analyzer.demonstrate_embeddings()\n",
        "analyzer.demonstrate_per_layer_embeddings()\n",
        "analyzer.analyze_model_architecture()\n",
        "\n",
        "# Create applications\n",
        "apps = Gemma3NApplications(analyzer)\n",
        "\n",
        "# Semantic search demo\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Deep learning uses neural networks with multiple layers.\",\n",
        "    \"Natural language processing helps computers understand human language.\",\n",
        "    \"Computer vision enables machines to interpret visual information.\",\n",
        "    \"Reinforcement learning teaches agents through rewards and penalties.\",\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"Quantum computing uses quantum mechanics principles.\",\n",
        "    \"Blockchain is a distributed ledger technology.\"\n",
        "]\n",
        "\n",
        "apps.semantic_search(\"What is deep learning?\", documents)\n",
        "apps.semantic_search(\"Tell me about NLP\", documents)\n",
        "\n",
        "# Text classification demo\n",
        "classification_texts = [\n",
        "    \"This movie was absolutely fantastic!\",\n",
        "    \"I really enjoyed watching this film.\",\n",
        "    \"Terrible movie, waste of time.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Machine learning is fascinating.\",\n",
        "    \"I love artificial intelligence.\"\n",
        "]\n",
        "\n",
        "apps.text_classification_demo(classification_texts)\n",
        "\n",
        "# ============================================================================\n",
        "# CONCLUSION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Gemma 3N Analysis Complete!\n",
        "\n",
        "What we learned:\n",
        "1. The tokenizer works perfectly - we can encode/decode text\n",
        "2. The embedder provides 2048-dimensional representations\n",
        "3. Per-layer embeddings show 30 layers with 256 dims each\n",
        "4. Vision adapter is available but needs the vision encoder\n",
        "\n",
        "Practical uses without the decoder:\n",
        "✓ Semantic search systems\n",
        "✓ Text similarity analysis\n",
        "✓ Document clustering\n",
        "✓ Feature extraction for ML\n",
        "✓ Text classification\n",
        "✓ Embedding-based retrieval\n",
        "\n",
        "The main limitation is the INT4-quantized decoder that won't load.\n",
        "Without it, we cannot generate text, but we can still extract\n",
        "meaningful representations for many NLP tasks.\n",
        "\n",
        "To get full text generation:\n",
        "→ Use a different model format (not .task)\n",
        "→ Use cloud APIs (Vertex AI, Gemini API)\n",
        "→ Wait for TFLite INT4 support\n",
        "→ Convert to FP16/INT8 format\n",
        "\"\"\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-24T20:12:46.866452Z",
          "iopub.execute_input": "2025-05-24T20:12:46.868236Z",
          "iopub.status.idle": "2025-05-24T20:12:47.217788Z",
          "shell.execute_reply.started": "2025-05-24T20:12:46.868164Z",
          "shell.execute_reply": "2025-05-24T20:12:47.215868Z"
        },
        "id": "bXzLOLkayUoU"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
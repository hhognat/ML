{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhognat/ML/blob/main/Efficient_Fine_Tuning_Llama2_7B_for_Python_Code_Generation_Marktechpost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate\n",
        "!pip install -q peft\n",
        "!pip install -q transformers\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-10T04:27:48.430457Z",
          "iopub.execute_input": "2024-10-10T04:27:48.431497Z",
          "iopub.status.idle": "2024-10-10T04:28:37.814207Z",
          "shell.execute_reply.started": "2024-10-10T04:27:48.431454Z",
          "shell.execute_reply": "2024-10-10T04:28:37.813257Z"
        },
        "trusted": true,
        "id": "tYaj6Foygf76",
        "outputId": "189a43a8-8fe1-4a0b-8987-67ed13e1a897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m833.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.3/318.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:28:46.173039Z",
          "iopub.execute_input": "2024-10-10T04:28:46.173870Z",
          "iopub.status.idle": "2024-10-10T04:28:46.180052Z",
          "shell.execute_reply.started": "2024-10-10T04:28:46.173829Z",
          "shell.execute_reply": "2024-10-10T04:28:46.179161Z"
        },
        "trusted": true,
        "id": "Pj5rrqxogf8A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"nikhiljatiwal/minipython-Alpaca-14k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"/kaggle/working/llama-2-7b-codeAlpaca\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:28:48.744973Z",
          "iopub.execute_input": "2024-10-10T04:28:48.745863Z",
          "iopub.status.idle": "2024-10-10T04:28:48.751294Z",
          "shell.execute_reply.started": "2024-10-10T04:28:48.745821Z",
          "shell.execute_reply": "2024-10-10T04:28:48.750352Z"
        },
        "trusted": true,
        "id": "r9oWFNurgf8B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:28:51.137168Z",
          "iopub.execute_input": "2024-10-10T04:28:51.137545Z",
          "iopub.status.idle": "2024-10-10T04:28:51.143730Z",
          "shell.execute_reply.started": "2024-10-10T04:28:51.137511Z",
          "shell.execute_reply": "2024-10-10T04:28:51.142729Z"
        },
        "trusted": true,
        "id": "JDLEjz3bgf8B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ################################################################################\n",
        "# # bitsandbytes parameters\n",
        "# ################################################################################\n",
        "\n",
        "# # Activate 4-bit precision base model loading\n",
        "# use_4bit = True\n",
        "\n",
        "# # Compute dtype for 4-bit base models\n",
        "# bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# # Quantization type (fp4 or nf4)\n",
        "# bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# # Activate nested quantization for 4-bit base models (double quantization)\n",
        "# use_nested_quant = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-09T20:56:01.718142Z",
          "iopub.execute_input": "2024-10-09T20:56:01.719039Z",
          "iopub.status.idle": "2024-10-09T20:56:01.723720Z",
          "shell.execute_reply.started": "2024-10-09T20:56:01.719000Z",
          "shell.execute_reply": "2024-10-09T20:56:01.722769Z"
        },
        "trusted": true,
        "id": "S-jN6T3Jgf8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"/kaggle/working/llama-2-7b-codeAlpaca\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16 training (set to True for mixed precision training)\n",
        "fp16 = True\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 8\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 8\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient norm (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"adamw_torch\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Group sequences into batches with the same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Ratio of steps for a linear warmup\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 100\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 10"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:28:54.602803Z",
          "iopub.execute_input": "2024-10-10T04:28:54.603298Z",
          "iopub.status.idle": "2024-10-10T04:28:54.612278Z",
          "shell.execute_reply.started": "2024-10-10T04:28:54.603253Z",
          "shell.execute_reply": "2024-10-10T04:28:54.611195Z"
        },
        "trusted": true,
        "id": "2RsC70jtgf8C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"CUDA Version:\", torch.version.cuda)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-09T22:14:29.846923Z",
          "iopub.execute_input": "2024-10-09T22:14:29.847859Z",
          "iopub.status.idle": "2024-10-09T22:14:29.853284Z",
          "shell.execute_reply.started": "2024-10-09T22:14:29.847817Z",
          "shell.execute_reply": "2024-10-09T22:14:29.852128Z"
        },
        "trusted": true,
        "id": "uZLj8kFZgf8D",
        "outputId": "d625ab2a-aaf0-44cc-f577-e78a82820990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.5.1+cu124\n",
            "CUDA Version: 12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-09T22:14:31.408978Z",
          "iopub.execute_input": "2024-10-09T22:14:31.409671Z",
          "iopub.status.idle": "2024-10-09T22:14:32.516733Z",
          "shell.execute_reply.started": "2024-10-09T22:14:31.409630Z",
          "shell.execute_reply": "2024-10-09T22:14:32.515604Z"
        },
        "trusted": true,
        "id": "hvF6Raxmgf8E",
        "outputId": "c25e71ca-365b-44e4-ee3f-38820d62f11e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:30:12.045759Z",
          "iopub.execute_input": "2024-10-10T04:30:12.046570Z",
          "iopub.status.idle": "2024-10-10T04:30:12.052007Z",
          "shell.execute_reply.started": "2024-10-10T04:30:12.046531Z",
          "shell.execute_reply": "2024-10-10T04:30:12.051043Z"
        },
        "trusted": true,
        "id": "hFqOosOFgf8E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load base model with 8-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Prepare model for training\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-09T22:14:36.072694Z",
          "iopub.execute_input": "2024-10-09T22:14:36.073776Z",
          "iopub.status.idle": "2024-10-09T22:15:46.472456Z",
          "shell.execute_reply.started": "2024-10-09T22:14:36.073721Z",
          "shell.execute_reply": "2024-10-09T22:15:46.471423Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "PEhYHylugf8F",
        "outputId": "564e78da-bc66-485f-fac4-48714cef1d47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DatasetNotFoundError",
          "evalue": "Dataset 'user/minipython-Alpaca-14k' doesn't exist on the Hub or cannot be accessed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1b6af73cd973>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2130\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m                 ) from e\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                 dataset_script_path = api.hf_hub_download(\n",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'user/minipython-Alpaca-14k' doesn't exist on the Hub or cannot be accessed."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:30:19.277709Z",
          "iopub.execute_input": "2024-10-10T04:30:19.278572Z",
          "iopub.status.idle": "2024-10-10T04:30:19.283900Z",
          "shell.execute_reply.started": "2024-10-10T04:30:19.278531Z",
          "shell.execute_reply": "2024-10-10T04:30:19.282876Z"
        },
        "trusted": true,
        "id": "mo5qW8cQgf8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-09T22:15:57.690157Z",
          "iopub.execute_input": "2024-10-09T22:15:57.690889Z",
          "iopub.status.idle": "2024-10-09T22:16:12.477252Z",
          "shell.execute_reply.started": "2024-10-09T22:15:57.690851Z",
          "shell.execute_reply": "2024-10-09T22:16:12.476257Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "a38b9772afdc4cf794e9ffb612153727"
          ]
        },
        "id": "7-wUixjFgf8G",
        "outputId": "407a36cc-5bfd-4a04-8433-b5087aae984f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a38b9772afdc4cf794e9ffb612153727"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-09T22:16:19.459428Z",
          "iopub.execute_input": "2024-10-09T22:16:19.460134Z",
          "iopub.status.idle": "2024-10-10T03:19:39.768512Z",
          "shell.execute_reply.started": "2024-10-09T22:16:19.460086Z",
          "shell.execute_reply": "2024-10-10T03:19:39.767691Z"
        },
        "trusted": true,
        "id": "9b827kubgf8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run text generation pipeline with the fine-tuned model\n",
        "prompt = \"How can I write a Python program that calculates the mean, standard deviation, and coefficient of variation of a dataset from a CSV file?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=400)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T03:51:12.422617Z",
          "iopub.execute_input": "2024-10-10T03:51:12.423015Z",
          "iopub.status.idle": "2024-10-10T03:52:21.808630Z",
          "shell.execute_reply.started": "2024-10-10T03:51:12.422976Z",
          "shell.execute_reply": "2024-10-10T03:52:21.807555Z"
        },
        "trusted": true,
        "id": "xTZdiZzBgf8H",
        "outputId": "8282bb60-4ac9-458e-b65b-b507ef1bdeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<s>[INST] How can I write a Python program that calculates the mean, standard deviation, and coefficient of variation of a dataset from a CSV file? [/INST] You can use the following code to calculate the mean, standard deviation, and coefficient of variation of a dataset from a CSV file:\n\n```python\nimport pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean\nmean = df.mean()\n\n# Calculate the standard deviation\nstd = df.std()\n\n# Calculate the coefficient of variation\ncv = df.div(df.mean())\n\nprint(\"Mean:\", mean)\nprint(\"Standard Deviation:\", std)\nprint(\"Coefficient of Variation:\", cv)\n```\n\nIn this code, we first import the pandas library to load the CSV file. We then use the `read_csv()` function to load the data from the CSV file into a pandas DataFrame.\n\nNext, we calculate the mean by using the `mean()` function of the DataFrame. We calculate the standard deviation by using the `std()` function of the DataFrame.\n\nFinally, we calculate the coefficient of variation by dividing the DataFrame by its mean using the `div()` function. The coefficient of variation is then printed to the console.\n\nBy running this code, you will get the mean, standard deviation, and coefficient of variation of the dataset from the CSV file. You can modify the CSV file path to match your own dataset.\n\nNote: This code assumes that the CSV file contains numerical data. If the CSV file contains non-numerical data, you may need to modify the code accordingly. Additionally, the CSV file should have a header row containing the column names. If the CSV file does not have a header row, you may need to add\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:02:53.071555Z",
          "iopub.execute_input": "2024-10-10T04:02:53.072429Z",
          "iopub.status.idle": "2024-10-10T04:02:53.187044Z",
          "shell.execute_reply.started": "2024-10-10T04:02:53.072378Z",
          "shell.execute_reply": "2024-10-10T04:02:53.186328Z"
        },
        "trusted": true,
        "id": "j6tkMSOogf8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "# del model\n",
        "# del pipe\n",
        "# del trainer\n",
        "# del dataset\n",
        "del tokenizer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:39:08.011127Z",
          "iopub.execute_input": "2024-10-10T04:39:08.011530Z",
          "iopub.status.idle": "2024-10-10T04:39:08.823692Z",
          "shell.execute_reply.started": "2024-10-10T04:39:08.011493Z",
          "shell.execute_reply": "2024-10-10T04:39:08.822729Z"
        },
        "trusted": true,
        "id": "ZYcUzaazgf8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check the number of GPUs available\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "# Check if CUDA device 1 is available\n",
        "if num_gpus > 1:\n",
        "    print(\"cuda:1 is available.\")\n",
        "else:\n",
        "    print(\"cuda:1 is not available.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:43:01.022838Z",
          "iopub.execute_input": "2024-10-10T04:43:01.024210Z",
          "iopub.status.idle": "2024-10-10T04:43:01.032322Z",
          "shell.execute_reply.started": "2024-10-10T04:43:01.024148Z",
          "shell.execute_reply": "2024-10-10T04:43:01.031111Z"
        },
        "trusted": true,
        "id": "RUDkm-S2gf8H",
        "outputId": "1b96ffbe-fa19-48d6-dae9-2a3a6d861568"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of GPUs available: 2\ncuda:1 is available.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Specify the device ID for your desired GPU (e.g., 0 for the first GPU, 1 for the second GPU)\n",
        "device_id = 1  # Change this based on your available GPUs\n",
        "device = f\"cuda:{device_id}\"\n",
        "\n",
        "# Load the base model on the specified GPU\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",  # Use auto to load on the available device\n",
        ")\n",
        "\n",
        "# Load the LoRA weights\n",
        "lora_model = PeftModel.from_pretrained(base_model, new_model)\n",
        "\n",
        "# Move LoRA model to the specified GPU\n",
        "lora_model.to(device)\n",
        "\n",
        "# Merge the LoRA weights with the base model weights\n",
        "model = lora_model.merge_and_unload()\n",
        "\n",
        "# Ensure the merged model is on the correct device\n",
        "model.to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-10T04:44:05.748951Z",
          "iopub.execute_input": "2024-10-10T04:44:05.749851Z",
          "iopub.status.idle": "2024-10-10T04:44:12.172850Z",
          "shell.execute_reply.started": "2024-10-10T04:44:05.749812Z",
          "shell.execute_reply": "2024-10-10T04:44:12.172079Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "0daa37b304844210bc3343bf9b284a3b"
          ]
        },
        "id": "xZfyT0LRgf8H",
        "outputId": "0d0607d3-9e23-4344-849a-05f4957fe9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0daa37b304844210bc3343bf9b284a3b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-C4naSSgf8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
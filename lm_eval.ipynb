{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdxE3cl/i8EBVG512k/ySE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhognat/ML/blob/main/lm_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6XUxV116JFEq"
      },
      "outputs": [],
      "source": [
        "!pip install -q lm_eval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export tasks=gsm8k\n",
        "!export model=\"google/gemma-3-12b-it\"\n",
        "!pip install -U \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qOF796Z_Mdeq",
        "outputId": "2d403cd8-e5dc-4f8e-ffff-202e821e3961"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.13.2)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.51)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2025.4.26)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2-s8hC2zYQMk",
        "outputId": "91566c36-1d28-45a7-96e2-8199ad9e0b18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `HF_TOKEN` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `HF_TOKEN`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf --model_args pretrained=\"google/gemma-3-12b-it,parallelize=True\" \\\n",
        "          --tasks gsm8k \\\n",
        "          --trust_remote_code \\\n",
        "          --limit 10 \\\n",
        "          --batch_size 5 --num_fewshot 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZSnbHz7MT7e",
        "outputId": "c55a7b28-2e98-42bc-d840-5d9350f385c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-13 20:42:27.539418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747168947.571589   16789 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747168947.579638   16789 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-13 20:42:27.605611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:lm_eval.__main__: --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "INFO:lm_eval.__main__:Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\n",
            "INFO:lm_eval.__main__:Selected Tasks: ['gsm8k']\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Initializing hf model, with arguments: {'pretrained': 'google/gemma-3-12b-it', 'parallelize': True, 'trust_remote_code': True}\n",
            "config.json: 100% 916/916 [00:00<00:00, 5.04MB/s]\n",
            "tokenizer_config.json: 100% 1.16M/1.16M [00:00<00:00, 19.1MB/s]\n",
            "tokenizer.model: 100% 4.69M/4.69M [00:00<00:00, 63.4MB/s]\n",
            "tokenizer.json: 100% 33.4M/33.4M [00:00<00:00, 148MB/s]\n",
            "added_tokens.json: 100% 35.0/35.0 [00:00<00:00, 122kB/s]\n",
            "special_tokens_map.json: 100% 662/662 [00:00<00:00, 4.32MB/s]\n",
            "INFO:lm_eval.models.huggingface:Model parallel was set to True, setting max memory per GPU to {} and device map to auto\n",
            "model.safetensors.index.json: 100% 109k/109k [00:00<00:00, 118MB/s]\n",
            "model-00001-of-00005.safetensors: 100% 4.98G/4.98G [00:39<00:00, 127MB/s]\n",
            "model-00002-of-00005.safetensors: 100% 4.93G/4.93G [00:43<00:00, 113MB/s]\n",
            "model-00003-of-00005.safetensors: 100% 4.93G/4.93G [01:03<00:00, 77.8MB/s]\n",
            "model-00004-of-00005.safetensors: 100% 4.93G/4.93G [01:04<00:00, 76.5MB/s]\n",
            "model-00005-of-00005.safetensors: 100% 4.60G/4.60G [01:00<00:00, 75.5MB/s]\n",
            "Loading checkpoint shards: 100% 5/5 [00:00<00:00, 599.72it/s]\n",
            "generation_config.json: 100% 215/215 [00:00<00:00, 1.22MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 389, in cli_evaluate\n",
            "    results = evaluator.simple_evaluate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 422, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 209, in simple_evaluate\n",
            "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 151, in create_from_arg_string\n",
            "    return cls(**args, **args2)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 191, in __init__\n",
            "    self._create_model(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 587, in _create_model\n",
            "    self._model = self.AUTO_MODEL_CLASS.from_pretrained(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4475, in from_pretrained\n",
            "    dispatch_model(model, **device_map_kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\", line 501, in dispatch_model\n",
            "    raise ValueError(\n",
            "ValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\n"
          ]
        }
      ]
    }
  ]
}
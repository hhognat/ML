{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hhognat/ML/blob/main/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "926bada6"
   },
   "source": [
    "Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "a110dfce"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkOtOq0jDE0c"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "      <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e624ec07"
   },
   "source": [
    "# Fine-tune Gemma 3 270M for emoji generation\n",
    "\n",
    "This notebook fine-tunes Gemma for the task of translating text into emoji using Quantized Low-Rank Adaptation (QLoRA) through the Hugging Face Transformer Reinfocement Learning ([TRL](https://huggingface.co/docs/trl/en/index)) library to help reduce memory usage and speed up the fine-tuning process.\n",
    "\n",
    "When training [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m) on a Google Colab T4 GPU accelerator, this process can take as little as 10 minutes end-to-end. Run each code snippet to:\n",
    "\n",
    "1. Set up the Colab environment\n",
    "2. Prepare a dataset for fine-tuning\n",
    "3. Load and test the base Gemma 3 270M model\n",
    "4. Fine-tune the model\n",
    "5. Test, evaluate, and save the model for further use\n",
    "\n",
    "## Set up development environment\n",
    "\n",
    "The first step is to install the necessary libraries using the `pip` package installer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BEK9IfKBqQaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: tensorboard in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (2.14.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (11.3.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: trl in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (4.3.0)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: protobuf==3.20.3 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (3.20.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch tensorboard emoji\n",
    "%pip install -U transformers trl datasets accelerate evaluate sentencepiece bitsandbytes protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTuW1LPfLXi9"
   },
   "source": [
    "You may have the restart your session (runtime) to use newly installed libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ef3d54b"
   },
   "source": [
    "##Enable Hugging Face permissions\n",
    "To use Gemma models, you'll need to accept the model usage license and create an Access Token:\n",
    "\n",
    "1. **Accept license** on the [model page](http://huggingface.co/google/gemma-3-270m-it).\n",
    "\n",
    "2. **Get a valid [Access Token](https://huggingface.co/settings/tokens) with 'Write' access (very important!)**\n",
    "\n",
    "3. Create a new Colab secret in the left toolbar. Specify `HF_TOKEN` as the 'Name', add your unique token as the 'Value', and toggle 'Notebook access' on."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "b6d79c93"
   },
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login into Hugging Face Hub\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42c60525"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "Hugging Face hosts a large collection of datasets for training and evaluating models. If you're not using a custom dataset, you can load a [premade dataset](https://huggingface.co/datasets/kr15t3n/g-emoji) containing examples of text and corresponding emoji translations.\n",
    "\n",
    "**If you'd like to use your own custom dataset, skip to the next step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bc3BYl72pWhp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's the 10th example from the dataset: {'text': 'A supernova just exploded', 'emoji': 'ðŸ’¥âœ¨'}\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Use the first 2000 samples for efficient training.\n",
    "general_dataset_path = load_dataset(\"kr15t3n/text2emoji\", encoding=\"utf-8\", split=\"train\")\n",
    "\n",
    "# Clean dataset to only use examples where 'emoji' field contains only emoji characters\n",
    "def is_only_emoji(sample):\n",
    "  emoji_string = sample['emoji']\n",
    "  if not emoji_string:\n",
    "    return False\n",
    "  return all(emoji.is_emoji(char) for char in emoji_string)\n",
    "dataset = general_dataset_path.filter(is_only_emoji)\n",
    "\n",
    "print(f\"\\nHere's the 10th example from the dataset: {dataset[10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc1rhhdFGwNu"
   },
   "source": [
    "### Upload a custom dataset\n",
    "**If you've already loaded a dataset, skip this step.**\n",
    "\n",
    "\n",
    "You can customize Gemma 3 270M to use specific emoji by creating a spreadsheet containing your text-to-emoji dataset structured as key-value pairs. If you want to encourage memorization of specific emoji, we recommend providing 10-20 examples of that emoji with different text variations.\n",
    "\n",
    "Use a [premade dataset](https://github.com/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Emoji%20Translation%20Dataset%20-%20Dataset.csv)  as a template to create your own dataset, then upload it the Files folder in the left toolbar. Get its path by right-clicking the file and pointing to it in `custom_dataset_path`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "g0HGgZj_3f7g"
   },
   "source": [
    "import emoji\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Point to your uploaded dataset\n",
    "custom_dataset_path = \"/content/Emoji Translation Dataset - Dataset.csv\"      #@param {type:\"string\"}\n",
    "dataset = load_dataset(\"csv\", data_files=custom_dataset_path, encoding=\"utf-8\", split=\"train\")\n",
    "\n",
    "# Clean dataset to only use examples where 'emoji' field contains only emoji characters\n",
    "def is_only_emoji(sample):\n",
    "  emoji_string = sample['emoji']\n",
    "  if not emoji_string:\n",
    "    return False\n",
    "  return emoji.purely_emoji(emoji_string)\n",
    "dataset = dataset.filter(is_only_emoji)\n",
    "\n",
    "print(f\"\\nHere's the 10th example from your dataset: {dataset[10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0eb2e06"
   },
   "source": [
    "## Load the model\n",
    "\n",
    "You can access [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m-it) from Hugging Face Hub by accepting the license terms. The instruction-tuned version of the model has already been trained on how to follow directions and with fine-tuning, you'll now adapt it to a new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "18069ed2"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dispatch_model' from partially initialized module 'accelerate.big_modeling' (most likely due to a circular import) (/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/big_modeling.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m      4\u001b[39m gemma_model = \u001b[33m\"\u001b[39m\u001b[33mgoogle/gemma-3-270m-it\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m, attn_implementation=\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.bfloat16)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:40\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     cached_file,\n\u001b[32m     33\u001b[39m     extract_commit_hash,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     logging,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     CONFIG_MAPPING_NAMES,\n\u001b[32m     43\u001b[39m     AutoConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     replace_list_option_in_docstrings,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     46\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     48\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/transformers/generation/utils.py:120\u001b[39m\n\u001b[32m    117\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlignDevicesHook, add_hook_to_module\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Variable names used to hold the cache at generation time\u001b[39;00m\n\u001b[32m    124\u001b[39m ALL_CACHE_NAMES = [\n\u001b[32m    125\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# default\u001b[39;00m\n\u001b[32m    126\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcache_params\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# mamba-based models\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpast_buckets_states\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# reformer\u001b[39;00m\n\u001b[32m    130\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/__init__.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m1.11.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     cpu_offload,\n\u001b[32m     19\u001b[39m     cpu_offload_with_hook,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     load_checkpoint_and_dispatch,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m skip_first_batches\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/accelerator.py:37\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_torch_state_dict_into_shards\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FP8BackendType\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _attach_context_parallel_hooks\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpointing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/big_modeling.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     AlignDevicesHook,\n\u001b[32m     27\u001b[39m     CpuOffload,\n\u001b[32m     28\u001b[39m     LayerwiseCastingHook,\n\u001b[32m     29\u001b[39m     UserCpuOffloadHook,\n\u001b[32m     30\u001b[39m     add_hook_to_module,\n\u001b[32m     31\u001b[39m     attach_align_device_hook,\n\u001b[32m     32\u001b[39m     attach_align_device_hook_on_blocks,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     OffloadedWeightsLoader,\n\u001b[32m     36\u001b[39m     check_cuda_p2p_ib_support,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     retie_parameters,\n\u001b[32m     52\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SUPPORTED_PYTORCH_LAYERS_FOR_UPCASTING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/hooks.py:23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PartialState\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     PrefixedDataset,\n\u001b[32m     25\u001b[39m     find_device,\n\u001b[32m     26\u001b[39m     named_module_tensors,\n\u001b[32m     27\u001b[39m     send_to_device,\n\u001b[32m     28\u001b[39m     set_module_tensor_to_device,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     is_mlu_available,\n\u001b[32m     32\u001b[39m     is_musa_available,\n\u001b[32m     33\u001b[39m     is_npu_available,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clear_device_cache\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/utils/__init__.py:224\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_deepspeed_available():\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    214\u001b[39m         DeepSpeedEngineWrapper,\n\u001b[32m    215\u001b[39m         DeepSpeedOptimizerWrapper,\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m         map_pytorch_optim_to_deepspeed,\n\u001b[32m    222\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbnb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    226\u001b[39m     disable_fsdp_ram_efficient_loading,\n\u001b[32m    227\u001b[39m     enable_fsdp_ram_efficient_loading,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     save_fsdp_optimizer,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlaunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    242\u001b[39m     PrepareForLaunch,\n\u001b[32m    243\u001b[39m     _filter_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m     prepare_tpu,\n\u001b[32m    249\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/utils/bnb.py:29\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     is_4bit_bnb_available,\n\u001b[32m     26\u001b[39m     is_8bit_bnb_available,\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch_model, init_empty_weights\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BnbQuantizationConfig\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     find_tied_parameters,\n\u001b[32m     33\u001b[39m     get_balanced_memory,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     set_module_tensor_to_device,\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'dispatch_model' from partially initialized module 'accelerate.big_modeling' (most likely due to a circular import) (/opt/anaconda3/envs/ds-3-12-11/lib/python3.12/site-packages/accelerate/big_modeling.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gemma_model = \"google/gemma-3-270m-it\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\"auto\", attn_implementation=\"eager\", dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "print(f\"Device: {base_model.device}\")\n",
    "print(f\"DType: {base_model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hI4twbrz0xj"
   },
   "source": [
    "Device should print as `cuda` if you're using a GPU runtime. **If you haven't aleady, use a free T4 GPU runtime in your Colab for faster fine-tuning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PafivP8u1Gv9"
   },
   "source": [
    "### Format the training dataset\n",
    "Now that you've loaded your data, format the training dataset into conversational roles, including the text input and emoji output, plus a system prompt that contains the direction to the model. This helps the model learn how to interpret the 'text' and 'emoji' columns from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWz32s5h074E"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def translate(sample):\n",
    "  return {\n",
    "      \"messages\": [\n",
    "          {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
    "          {\"role\": \"user\", \"content\": f\"{sample['text']}\"},\n",
    "          {\"role\": \"assistant\", \"content\": f\"{sample['emoji']}\"}\n",
    "      ]\n",
    "  }\n",
    "\n",
    "training_dataset = dataset.map(translate, remove_columns=dataset.features.keys())\n",
    "training_dataset_splits = training_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "\n",
    "print(\"\\nHere's the 40th example from the formatted training dataset:\")\n",
    "print(training_dataset[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3w3b9-O4fDz"
   },
   "source": [
    "### Recommended: Test the base model\n",
    "\n",
    "Let's first check how the base model's ability to respond to the instruction \"Translate this text to emoji\"\n",
    "\n",
    "Try testing it a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8L0_INJyUok"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "# Create a transformers inference pipeline\n",
    "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
    "\n",
    "# Select a random sample from the test dataset\n",
    "rand_idx = randint(0, len(training_dataset_splits[\"test\"]) - 1)\n",
    "test_sample = training_dataset_splits[\"test\"][rand_idx]\n",
    "\n",
    "# Handle messages\n",
    "all_messages = test_sample['messages']\n",
    "user_message_content = next((msg['content'].strip() for msg in all_messages if msg['role'] == 'user'), \"Not Found\")\n",
    "dataset_emoji = next((msg['content'].strip() for msg in all_messages if msg['role'] == 'assistant'), \"Not Found\")\n",
    "prompt_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
    "    {\"role\": \"user\", \"content\": user_message_content}\n",
    "]\n",
    "\n",
    "# Apply the chat template. This will format the messages correctly for the model.\n",
    "prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate the output\n",
    "output = pipe(prompt, max_new_tokens=64)\n",
    "model_output_only = output[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "print(f\"\\nDataset text: {user_message_content}\")\n",
    "print(f\"\\nDataset emoji: {dataset_emoji}\")\n",
    "print(f\"\\nModel generated output: {model_output_only}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph26HDJgua3W"
   },
   "source": [
    "The base model output may not meet your expectationsâ€”and that's okay!\n",
    "\n",
    "Gemma 3 270M was designed for task specialization, which means it can improve performance for specific tasks when trained with representative examples. Let's fine-tune the model for more reliable outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbd9fc1b"
   },
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Hugging Face [TRL](https://huggingface.co/docs/trl/index) provides tools for training and fine-tuning LLMs using memory-efficient techniques like QLoRA (Quantized Low-Rank Adaptation) to train adapters on top of a frozen quantized version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BJFoOdL0y8w"
   },
   "source": [
    "### Configure the tuning job\n",
    "Define the training configuration for the Gemma 3 base model:\n",
    "\n",
    "1. `BitsandBytesConfig` to quantize the model for memory efficiency\n",
    "2. `LoraConfig` for parameter-efficient fine-tuning\n",
    "2. `SFTConfig` for supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiIj1ADc-exw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "\n",
    "adapter_path = \"/content/myemoji-gemma-adapters\"      # Where to save your LoRA adapters\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",                      # Target all linear layers\n",
    "    lora_dropout=0.05,                                # Increase to 0.1 to induce overfitting\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"]       # Save the lm_head and embed_tokens as you train the special tokens\n",
    ")\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=adapter_path,                          # Directory to save adapters\n",
    "    num_train_epochs=3,                               # Number of training epochs\n",
    "    per_device_train_batch_size=4,                    # Batch size per device during training\n",
    "    logging_strategy=\"epoch\",                         # Log every epoch\n",
    "    eval_strategy=\"epoch\",                            # Evaluate loss metrics every epoch\n",
    "    save_strategy=\"epoch\",                            # Save checkpoint every epoch\n",
    "    learning_rate=5e-5,                               # Learning rate,\n",
    "    lr_scheduler_type=\"constant\",                     # Use constant learning rate scheduler\n",
    "    max_length=256,                                   # Max sequence length for model and packing of the dataset\n",
    "    gradient_checkpointing=False,                     # Use gradient checkpointing to save memory\n",
    "    packing=False,                                    # Groups multiple samples in the dataset into a single sequence\n",
    "    optim=\"adamw_torch_fused\",                        # Use fused adamw optimizer\n",
    "    report_to=\"tensorboard\",                          # Report metrics to tensorboard\n",
    "    weight_decay=0.01,                                # Added weight decay for regularization\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, quantization_config=bnb_config, device_map=\"auto\", attn_implementation='eager')\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Training configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd88e798"
   },
   "source": [
    "### Start training\n",
    "\n",
    "`SFTTrainer` tokenizes the datasets and trains the base model using the hyperparameters from the previous step.\n",
    "\n",
    "The training time varies based on a range of factors, such as the size of your dataset or number of epochs. Using a T4 GPU, this takes about 10 minutes for 1000 training examples. If training is going slowly, check that you're using a T4 GPU in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqacJNeU9v7b"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Set training and evaluation datasets\n",
    "train_dataset = training_dataset_splits['train']\n",
    "eval_dataset = training_dataset_splits['test']\n",
    "\n",
    "# Train and save the LoRA adapters\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(adapter_path)\n",
    "\n",
    "print(f\"LoRA adapters saved to {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDvGlb5xO34z"
   },
   "source": [
    "The LoRA adapters for each training checkpoint (epoch) will be saved in your temporary Colab session storage. Now, you can evaluate the training and validation loss metrics to choose which adapters to merge with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xll8zZ3_u8Mt"
   },
   "source": [
    "### Plot training results\n",
    "To evaluate the model, you can plot the training and validation losses using Matplotlib to visualize these metrics over training steps or epochs. This helps monitor the training process and make informed decisions about hyperparameters or early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPN-DTopaUIy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access the log history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training / validation loss\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyIwS-orvWzd"
   },
   "source": [
    "Training loss measures the error on the data the model was trained on. Validation loss measures the error on a separate dataset the model has not seen before. Monitoring both helps detect overfitting (when the model performs well on training data but poorly on unseen data).\n",
    "\n",
    "- validation loss >> training loss: **overfitting**\n",
    "- validation loss > training loss: **some overfitting**\n",
    "- validation loss < training loss: **some underfitting**\n",
    "- validation loss << training loss: **underfitting**\n",
    "\n",
    "If your task requires memorization of specific examples, or specific emoji to be generated for a given text, overfitting can be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILgEZvZ71Edz"
   },
   "source": [
    "### Merge the adapters\n",
    "\n",
    "Once trained you can merge the LoRA adapters with the model. You can choose which adapters to merge by specifying the training checkpoint folder, otherwise it will default to the last epoch.\n",
    "* For better task generalization, choose the most underfit checkpoint (validation loss < training loss)\n",
    "* For better memorization of specific examples, choose the most overfit (checkpoint > training loss)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7e5BQ9U06Q2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"/content/myemoji-gemma-adapters/\"                 # Choose which adapters to merge, otherwise defaults to latest\n",
    "merged_model_path = \"/content/myemoji-gemma-merged/\"              # Location of merged model directory\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "\n",
    "# Load and merge the PEFT adapters onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model and its tokenizer\n",
    "model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "print(f\"Model merged and saved to {merged_model_path}. Final model vocabulary size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf86e31d"
   },
   "source": [
    "### Test the fine-tuned model\n",
    "\n",
    "Let's compare your fine-tuned model performance against the base model! Test a few inputs by updating `text_to_translate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28R3pRN_hai7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Create Transformers inference pipeline\n",
    "merged_model = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "pipe_base = pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\")\n",
    "\n",
    "# Test a prompt\n",
    "text_to_translate = \"let's go to the beach\"  #@param {type:\"string\"}\n",
    "inference_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate this text to emoji: \"},\n",
    "    {\"role\": \"user\", \"content\": text_to_translate}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(inference_messages, tokenize=False, add_generation_prompt=True)\n",
    "output = pipe(prompt, max_new_tokens=128)\n",
    "output_base = pipe_base(prompt, max_new_tokens=128)\n",
    "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
    "model_output_base = output_base[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "print(f\"\\nFine-tuned model output: {model_output}\")\n",
    "\n",
    "print(f\"\\nBase model output: {model_output_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86qPcFbHH_kh"
   },
   "source": [
    "Does the model output the emoji you'd expect?\n",
    "\n",
    "If you're not getting the results you want, you can try [using different hyperparameters](#scrollTo=-BJFoOdL0y8w) to train the model, or updating your training dataset to contain more representative examples.\n",
    "\n",
    "Once you're happy with the results, you can save your model to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H12D9g4X_peV"
   },
   "source": [
    "## Save your model and upload to Hugging Face Hub\n",
    "**You now have a customized Gemma 3 270M model! ðŸŽ‰**\n",
    "\n",
    "Upload it to a repository on Hugging Face Hub so you easily share your model or access it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbeyDcpwi4IB"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData, whoami\n",
    "\n",
    "#@markdown Name your model\n",
    "model_name = \"myemoji\"                            #@param {type:\"string\"}\n",
    "\n",
    "username = whoami()['name']\n",
    "hf_repo_id = f\"{username}/{model_name}-gemma-3-270m-it\"\n",
    "\n",
    "repo_url = model.push_to_hub(hf_repo_id, create_repo=True, commit_message=\"Upload model\")\n",
    "tokenizer.push_to_hub(hf_repo_id)\n",
    "\n",
    "card_content = f\"\"\"\n",
    "---\n",
    "base_model: {gemma_model}\n",
    "tags:\n",
    "- text-generation\n",
    "- emoji\n",
    "- gemma\n",
    "---\n",
    "A fine-tuned model based on `{gemma_model}`.\"\"\"\n",
    "card = ModelCard(card_content)\n",
    "card.push_to_hub(hf_repo_id)\n",
    "\n",
    "print(f\"Uploaded to {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f8ff452"
   },
   "source": [
    "## Summary and next steps\n",
    "\n",
    "This notebook covered how to efficiently fine-tune Gemma 3 270M for emoji generation. Continue on to the conversion and quantization steps to get it ready for on-device deployment. You can follow the steps to either:\n",
    "\n",
    "1.  [Convert for use with MediaPipe LLM Inference API](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_LiteRT_for_MediaPipe_LLM_Inference_API.ipynb)\n",
    "2.  [Convert for use with Transformers.js via ONNX Runtime](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_ONNX.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
